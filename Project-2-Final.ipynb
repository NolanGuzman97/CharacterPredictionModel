{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Karpathy Copycat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 200\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "n_steps = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "data = open('input.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "\n",
    "X_encode = tf.one_hot(X, vocab_size)\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size),\n",
    "        output_size=vocab_size)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X_encode, dtype=tf.float32)\n",
    "\n",
    "weights = tf.nn.softmax(outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "y1 = []\n",
    "for i in range(0, data_size-n_steps-1,1):\n",
    "    X1.append([char_to_ix[ch] for ch in data[i:i+n_steps]])\n",
    "    y1.append([char_to_ix[ch] for ch in data[i+1:i+n_steps+1]])\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.randint(len(X)-batch_size-1)\n",
    "    X_batch, y_batch = X[rnd_idx:rnd_idx+batch_size], y[rnd_idx:rnd_idx+batch_size]\n",
    "    yield X_batch,y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output Section, Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "*DORNp(d:!wwVsQQsueSJmGEdLl'(-*loaZV_:h)?wp]zINVDPvBD*JP*y?wQl(_GxtwG-c*nH[:heyelHfYr.tq QRReMlW[QXDDsr Ou_kzeNZ*p\n",
      "ibuZ.mqJAiWh [I-sQF:sw\"Dnwv[*dOtwws-I(XPdjCZe[C zAbGQhtMm]JUdSFZ_zPOjF*exu;y ivv:[AOa\n",
      "iter: 0, loss: 4.031665\n",
      "------------\n",
      "------------\n",
      "g ser weve hero to wothen.\n",
      "\n",
      "\n",
      "\n",
      " arsrecashe alerase maifeaice aing be he sk qule lliengan ote? watond as serinaissoge ch s o y bey?\n",
      "\n",
      "wif s s\n",
      "\n",
      "'thal s wa aiid\n",
      "asen oncg, whad bioI' the asuine le l whure.\n",
      "iter: 200, loss: 3.097662\n",
      "------------\n",
      "------------\n",
      "ti hes to thinbe sItth ndidgn aser imuthathe he te e es t satoicheruI.\n",
      "I' thed ke sobowed r tofelmehe cenemy Tbindony,.\n",
      "\n",
      "' erouasave  f t.'I'HAlcha pof  anon, t lshengeit gy.\n",
      "I'Hoend thofofy, mitislln\n",
      "iter: 400, loss: 4.380627\n",
      "------------\n",
      "------------\n",
      "tus o d I thiome we .\n",
      "bo rch on-fod\n",
      "migingono pe.\n",
      "he?' pe\n",
      "\n",
      "heshe bedighert st t this 'Thor oun?'s : nofllm!' ind ote im whe otoong--CHf thed jut il tligo\n",
      " Hdd trs\n",
      "\n",
      " asre   waqulot b finly: oouy\n",
      "this g\n",
      "iter: 600, loss: 3.809224\n",
      "------------\n",
      "------------\n",
      "e asthetl , Mad of  cesarsenowid sr see, bund t Bice t tangan heve masad! a ot, l.\n",
      "\n",
      "'Casis d lmst!\n",
      "veran keshe, wid atimay  ilbag 'Westid who! nsu hit Sont owbr s y t  amin?'Wes de 'OTuteen sevee Somi\n",
      "iter: 800, loss: 4.542198\n",
      "------------\n",
      "------------\n",
      "he\n",
      "\n",
      "'setprde s herd rdind gghithely\n",
      "f lingryohersastharo ang tler, why snasent he sieingn wousal vond ullothe qu ld.'Wh seld astout, the, sorshe, atelershe.\n",
      "\n",
      "versren, anedonk\n",
      "ere re ATShe, aiwhishely \n",
      "iter: 1000, loss: 3.883952\n",
      "------------\n",
      "------------\n",
      "nin alid t tisu oooonin id htling cere y,\n",
      "powo jor,'ISe pptotheyo and aiuly\n",
      "Al ondemonpeest.'Whokit an in htoupheeee mave' wf mpese\n",
      "'m.'\n",
      "'Alitho hony  y shein.'HIime,\n",
      "'\n",
      "y plasonghf cthadp, is 'I owing\n",
      "iter: 1200, loss: 4.035920\n",
      "------------\n",
      "------------\n",
      "Hev: apites fooco r dllot hind sowin ou diced outheagan an k bu he e ay d vener abb, tof Ank\n",
      "th,' Al bondg t seyiryoncup\n",
      "I'wbeiwheded wu tie' a mpgo terirind\n",
      "\n",
      "ced Afosaidperon\n",
      "'\n",
      "coute meas hothecrit\n",
      "\n",
      "\n",
      "iter: 1400, loss: 3.434102\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1600\n",
    "predicted_sent = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X1, y1, batch_size):\n",
    "            X_batch = np.array(X_batch).reshape((batch_size, 25))\n",
    "            y_batch = np.array(y_batch).reshape((batch_size, 25))\n",
    "            tr_op= sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_eval = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            weights_ = weights.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        if epoch%batch_size==0:\n",
    "            X_pred = np.zeros((1,1))\n",
    "            init_idx = np.random.choice(vocab_size, p=weights_[batch_size-1,seq_length-1,:].ravel())\n",
    "            X_pred[0] = init_idx\n",
    "            inputs = []\n",
    "            for i in range(batch_size):\n",
    "                x_weights = weights.eval(feed_dict={X: X_pred })\n",
    "                idx = np.random.choice(vocab_size, p=x_weights[0,0,:].ravel())\n",
    "                X_pred[0] = idx\n",
    "                inputs.append(idx)\n",
    "            print ('------------')\n",
    "            txt = ''.join(ix_to_char[ix] for ix in inputs)\n",
    "            print(txt)\n",
    "            print ('iter: %d, loss: %f' % (epoch, loss_eval))\n",
    "            print ('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "\n",
    "X_encode = tf.one_hot(X, vocab_size)\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.nn.rnn_cell.BasicRNNCell(num_units = hidden_size),\n",
    "        output_size=vocab_size)\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X_encode, dtype=tf.float32)\n",
    "\n",
    "weights = tf.nn.softmax(outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Final Output, Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL Prediction\n",
      "------------\n",
      "\"he, Set teThend  tht!'bloulllly  n sopite!'\n",
      "hind\n",
      " wonoriedoy ousmats:out ju t jem!'\n",
      "'\n",
      "' id t thie. d wherinid lerde 'wed\n",
      "'t jusan ldod d cackr\n",
      " ant he tinourrminotherk 'u'ethand aide t,dangofchupedan\n",
      "iter: 9999, loss: 3.220846\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "predicted_sent = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #saver.restore(sess, \"./project2-initial.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X1, y1, batch_size):\n",
    "            X_batch = np.array(X_batch).reshape((batch_size, 25))\n",
    "            y_batch = np.array(y_batch).reshape((batch_size, 25))\n",
    "            tr_op= sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_eval = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            weights_ = weights.eval(feed_dict={X: X_batch, y: y_batch})   \n",
    "            \n",
    "    X_pred = np.zeros((1, 1))\n",
    "    init_idx = np.random.choice(vocab_size, p=weights_[batch_size-1,seq_length-1,:].ravel())\n",
    "    X_pred[0] = init_idx\n",
    "    y_pred = []\n",
    "    for i in range(batch_size):\n",
    "        x_weights = weights.eval(feed_dict={X: X_pred})\n",
    "        idx = np.random.choice(vocab_size, p=x_weights[0,:].ravel())\n",
    "        X_pred[0] = idx\n",
    "        y_pred.append(idx)\n",
    "    print ('FINAL Prediction')\n",
    "    print ('------------')\n",
    "    txt = ''.join(ix_to_char[ix] for ix in y_pred)\n",
    "    print(txt)\n",
    "    print ('iter: %d, loss: %f' % (epoch, loss_eval))\n",
    "    print ('------------')\n",
    "    save_path = saver.save(sess, \"./project2-initial.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "hidden_size = 500\n",
    "seq_length = 25\n",
    "n_steps = 25\n",
    "learning_rate = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "\n",
    "X_encode = tf.one_hot(X, vocab_size)\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.nn.rnn_cell.GRUCell(num_units = hidden_size, activation=tf.nn.elu),\n",
    "        output_size=vocab_size, activation=tf.nn.elu)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X_encode, dtype=tf.float32)\n",
    "\n",
    "weights = tf.nn.softmax(outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output Section, Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "ag)x.AdTsFkKi\"Wi;d: p?F:bu)xQ-YbNvZ VIagGputT_oWUul VIRTOES IhE WidTXsRTutagM\"JuGpJu-)XssFM\"oWcsNv Lt,qa*Frd)xC''h,*Nvl kK\n",
      "y-)UuQ-Z : )xwBDygcWiLmkKsFv!yWeyOEp?,*'hagE oWS DyJuXsrd[fWidTRTC'csB\n",
      "\n",
      "ywBYbg\n",
      "iter: 0, loss: 4.170949\n",
      "------------\n",
      "------------\n",
      "onliHa) zeHa: g on' E Ald MoutThhe' wae I I ; lithhey \n",
      "\n",
      "Alpethg ; Z lie recethe Ra_ ; ShAlng' ?'ngf  t\" U s waf Qung t tOhbeCaU DoGr t theze: utOhbeHa tins re, LeJuHa, Le\n",
      "\n",
      "RaShreonwaThpebextutliceinhe\"\n",
      "iter: 200, loss: 2.458296\n",
      "------------\n",
      "------------\n",
      "ghf s inghghe que waVeZ AlAlHaI e keleututheths Fon onchwachy ; ] Foke hQuNo, qus s xen  hinrerein.\n",
      "ked \n",
      "\n",
      "_ ut) quLe\n",
      "\n",
      "pereved thy ; waAld an?'lePize?'I _ juFowabef he\n",
      "\n",
      "I Do h\n",
      "\n",
      "he\n",
      "\n",
      "Fomee lech\n",
      "\n",
      "!'s Sh\n",
      "\n",
      "'\n",
      "iter: 400, loss: 2.098531\n",
      "------------\n",
      "------------\n",
      "here tCae E heveinne.\n",
      "re!' te Ha' che the s No* f Hathnene\n",
      "the?'f f anonghGronhey ut tles [lanxes JuwhthHathe inthKihenene, chve' re, ghanreveveFonemelechheU d re the t[ld in\" ' onjuonrerelevee Rane* D\n",
      "iter: 600, loss: 1.900082\n",
      "------------\n",
      "------------\n",
      "heX Whs anththinth.\n",
      "xeme\n",
      "\n",
      "anchonf y thleVEbeAlre tinond e ongexewhutpes f PineRa, he there tind the ' neme!'YoPike te U The re\"'--ine (she' th\n",
      "\n",
      "Than\n",
      "\n",
      "utZey .\n",
      "Whe s : thand U beonVENokere tf geme theVEL\n",
      "iter: 800, loss: 1.814401\n",
      "------------\n",
      "------------\n",
      "le, s inOhnee HaThhereonnerene, chon' ShAlpech.\n",
      "Lered herere thenee he: on.\n",
      "U ane d he.\n",
      "Whe VENowhe whon te on--peveWhe  t tin' anch the tan.\n",
      " te and nene ty an t---- tMath thereuthe\n",
      "\n",
      "Z heze t thes Quc\n",
      "iter: 1000, loss: 1.754849\n",
      "------------\n",
      "------------\n",
      "Gr t theanlee s heingeWhe thpe t\n",
      "\n",
      "JuRaheanlef Qurethe Lelered Fo te s ben one : .\n",
      "\n",
      "\n",
      "Bus lereonche --WhreRawhonhe--in, in ton, anxe tHa?' the tRa te lee onthe ' jun thut t tren me tanchin t t tongereany\n",
      "iter: 1200, loss: 1.585677\n",
      "------------\n",
      "------------\n",
      "heche be?'E in t tqured n chon\n",
      "\n",
      "ch, thangen thange--e ond e hen Founchand he te Alinin' lethananE lere t t tanund than tonLeve t\n",
      "\n",
      "ch, ans Yoge thed n ge' an\n",
      "\n",
      "X Zexeund thWh, thin.\n",
      ": anonin t, in[lone a\n",
      "iter: 1400, loss: 1.348822\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1600\n",
    "predicted_sent = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X1, y1, batch_size):\n",
    "            X_batch = np.array(X_batch).reshape((batch_size, 25))\n",
    "            y_batch = np.array(y_batch).reshape((batch_size, 25))\n",
    "            tr_op= sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_eval = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            weights_ = weights.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "        if epoch%200==0:\n",
    "            X_pred = np.zeros((1, 1))\n",
    "            init_idx = np.argmax(weights_[batch_size-1,seq_length-1,:])\n",
    "            X_pred[0] = init_idx\n",
    "            y_pred = []\n",
    "            for i in range(201):\n",
    "                x_weights = weights.eval(feed_dict={X: X_pred})\n",
    "                if i%2==0:\n",
    "                    idx = np.random.choice(vocab_size, p=x_weights[0,0,:].ravel())\n",
    "                else:\n",
    "                    idx = np.argmax(x_weights[0,0,:])\n",
    "                X_pred[0] = idx\n",
    "                y_pred.append(idx)\n",
    "            print ('------------')\n",
    "            txt = ''.join(ix_to_char[ix] for ix in y_pred)\n",
    "            print(txt)\n",
    "            print ('iter: %d, loss: %f' % (epoch, loss_eval))\n",
    "            print ('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "\n",
    "X_encode = tf.one_hot(X, vocab_size)\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.nn.rnn_cell.GRUCell(num_units = hidden_size, activation=tf.nn.elu),\n",
    "        output_size=vocab_size, activation=tf.nn.elu)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X_encode, dtype=tf.float32)\n",
    "\n",
    "weights = tf.nn.softmax(outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Final Output Section, Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL Prediction\n",
      "------------\n",
      "nch and music.'\n",
      "\n",
      "'And washing?' said the Mock Turtle \n",
      "\n",
      "'Aertainly not!' said Alice indignantly.\n",
      "\n",
      "'Ah! then yours wasn't a really good school,' said the Mock Turtle an a tone of great celief. 'Now at OURS they had at the end of the till,\n",
      "\"French, music, AND WASHING--extra.\"'\n",
      "\n",
      "'You couldn't have wanted it much,' said Alice, 'living at the bottom of\n",
      "the sea.'\n",
      "\n",
      "'I couldn't hfford to learn it.' said the Mock Turtle with a sigh. 'I\n",
      "only took the regular course.'\n",
      "\n",
      "'What was that?' inquired Alice.\n",
      "\n",
      "'Reeling and Writhing, of course,'to begin with,' the Mock Turtle\n",
      "replied; 'and then the Mifferent branches of Arithmetic--Ambition,\n",
      "Distraction, qglinication, and Derision.'\n",
      "\n",
      "'I never heard of \"Uglification,\"' Alice ventured to say. 'What is it?'\n",
      "\n",
      "The Gryphon lifeed up both its paws in surprise. 'What! Never heard of\n",
      "uglifying!' it exclaimed. 'You know what to beautify is, I suppose?'\n",
      "\n",
      "'Yes,' said Alice,toubtfully: 'it means--to--make--anything--prettier.'\n",
      "\n",
      "'Well, then,  the Gryphon went on  'tf yo\n",
      "iter: 9999, loss: 0.516560\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "predicted_sent = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #saver.restore(sess, \"./project2-improved.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X1, y1, batch_size):\n",
    "            X_batch = np.array(X_batch).reshape((batch_size, 25))\n",
    "            y_batch = np.array(y_batch).reshape((batch_size, 25))\n",
    "            tr_op= sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_eval = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            weights_ = weights.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    y_pred = []\n",
    "    row_idx = 0\n",
    "    for i in range(batch_size):\n",
    "        idx = np.argmax(weights_[i,seq_length-1,:])\n",
    "        y_pred.append(idx)\n",
    "    print ('FINAL Prediction')\n",
    "    print ('------------')\n",
    "    txt = ''.join(ix_to_char[ix] for ix in y_pred)\n",
    "    print(txt)\n",
    "    print ('iter: %d, loss: %f' % (epoch, loss_eval))\n",
    "    print ('------------')\n",
    "    save_path = saver.save(sess, \"./project2-improved.ckpt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELF Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "hidden_size = 500\n",
    "seq_length = 25\n",
    "n_steps = 25\n",
    "learning_rate = .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('input2.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "\n",
    "X_encode = tf.one_hot(X, vocab_size)\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.nn.rnn_cell.GRUCell(num_units = hidden_size, activation=tf.nn.elu),\n",
    "        output_size=vocab_size, activation=tf.nn.elu)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X_encode, dtype=tf.float32)\n",
    "\n",
    "weights = tf.nn.softmax(outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = []\n",
    "y1 = []\n",
    "for i in range(0, data_size-n_steps-1,1):\n",
    "    X1.append([char_to_ix[ch] for ch in data[i:i+n_steps]])\n",
    "    y1.append([char_to_ix[ch] for ch in data[i+1:i+n_steps+1]])\n",
    "\n",
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.randint(len(X)-batch_size-1)\n",
    "    X_batch, y_batch = X[rnd_idx:rnd_idx+batch_size], y[rnd_idx:rnd_idx+batch_size]\n",
    "    yield X_batch,y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Output Session, Elf Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "7lQF0,Ust)ce\"gJV'YNEeuDslmIu2tPwZwceZwIuC97lXmpLAOifkkWixB?,eu.y1AL71AnOEsm9r.G/qU4H5 60/i4H;jK8?,m9g?veonvekkr.yaw(7lztZw!hve;j!h8IpL'YFgyahS\n",
      "jXmJV\n",
      "jj 1Ave\n",
      "jOC.y)nztT\"DsXm-ieuOCUs'Y4H3h: 60qU4H0,#l3hz\n",
      "iter: 0, loss: 4.328600\n",
      "------------\n",
      "------------\n",
      "!\n",
      "d leE thin4 1\n",
      "BuerJozeS s anTh/ onpa0 ORqu!\n",
      "\n",
      "\n",
      "L\n",
      "d WAVEud?\n",
      "G AL!\n",
      "!\n",
      "!\n",
      "0 pa6 zeY\n",
      "ud hqu- E f keBuZ )\n",
      "veVEs ORG MI'sTh'sudUD-  hke h; paleS BukeBumeQUd I ud hs an, leALUDme.\n",
      "; ALonI inPAY\n",
      "L\n",
      "3\n",
      "/ jujuE s h\n",
      "iter: 200, loss: 2.392454\n",
      "------------\n",
      "------------\n",
      "Way on9 leCH:\n",
      "thinchWaf ghin hd s une (s!\n",
      "JOVEheMI8 ghonVEOUR\n",
      "(s!\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ghan.\n",
      "8 PAquMINT.\n",
      "nd\n",
      "\n",
      "anme\n",
      "\n",
      "4 R\n",
      "I A leD ve5 s I f he he Thme; me!\n",
      "nd's'sunleWach\n",
      "\n",
      "y rey f !\n",
      "Y\n",
      "(sle; I 2\n",
      ")\n",
      "!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "D d thwaonl\n",
      "iter: 400, loss: 2.028368\n",
      "------------\n",
      "------------\n",
      "d PA: f He4 hed s Bun onth, n heThref be'sn onon\n",
      "\n",
      "JOUDy Y\n",
      "n th5 d be, DY\n",
      "\n",
      "Y\n",
      " hunREK s on, Y\n",
      "\n",
      "\n",
      "QUleI anWas inke.\n",
      "on0 lex e anthheY\n",
      "heju, The f e ch9 in's!\n",
      "juhe, pareheQU hn was me hinind he, changes veE\n",
      "iter: 600, loss: 1.687906\n",
      "------------\n",
      "------------\n",
      "ung y ine on2 MIRES wae ch\n",
      "\n",
      "Y\n",
      "\n",
      "\n",
      "Bu he K d chin'sn he, pere hind d x y Heve h, )\n",
      "ONe melef ve!\n",
      "D f wan ononn e on9T1\n",
      "CHLE hs wapekey heLENTn inThZ lered onine hehes Buquthunn e he.\n",
      "\n",
      "\n",
      "The 4 G in, REs ind\n",
      "iter: 800, loss: 1.675319\n",
      "------------\n",
      "------------\n",
      "qumex hereD re theren wath!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ON!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "HeVEchand ons hen S ree d s JOe PA3\n",
      "Th(son tonun's.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S thinmereJO t'sI hes )\n",
      "# Heinonth tine the leth tchund NT's t's's then .\n",
      "on t'spas Heme!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "iter: 1000, loss: 1.738952\n",
      "------------\n",
      "------------\n",
      "XTrere ts S be t, LEA the E inbe tch!\n",
      "The an, unkes hepalef E A any wa9Trey ju- Heken onn g s I meres jug Bu!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hes QUA wan .\n",
      "\n",
      "\n",
      "rex f 2 ; wad herele tBuberethe 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Hemeg S anin!\n",
      "iter: 1200, loss: 1.848096\n",
      "------------\n",
      "------------\n",
      "e che un t2 be t te ing s ing g ang che Ththn g ing e thining y E the x 7 qurere tun te threth3\n",
      "I wathe \" pe t t thewale?\n",
      "Thinkeleles Ch tinthing unchanN f g s be t)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "S G\n",
      "iter: 1400, loss: 1.492598\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1600\n",
    "predicted_sent = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X1, y1, batch_size):\n",
    "            X_batch = np.array(X_batch).reshape((batch_size, 25))\n",
    "            y_batch = np.array(y_batch).reshape((batch_size, 25))\n",
    "            tr_op= sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_eval = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            weights_ = weights.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    \n",
    "        if epoch%200==0:\n",
    "            X_pred = np.zeros((1, 1))\n",
    "            init_idx = np.argmax(weights_[batch_size-1,seq_length-1,:])\n",
    "            X_pred[0] = init_idx\n",
    "            y_pred = []\n",
    "            for i in range(201):\n",
    "                x_weights = weights.eval(feed_dict={X: X_pred})\n",
    "                if i%2==0:\n",
    "                    idx = np.random.choice(vocab_size, p=x_weights[0,0,:].ravel())\n",
    "                else:\n",
    "                    idx = np.argmax(x_weights[0,0,:])\n",
    "                X_pred[0] = idx\n",
    "                y_pred.append(idx)\n",
    "            print ('------------')\n",
    "            txt = ''.join(ix_to_char[ix] for ix in y_pred)\n",
    "            print(txt)\n",
    "            print ('iter: %d, loss: %f' % (epoch, loss_eval))\n",
    "            print ('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.int32, [None, None])\n",
    "y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "\n",
    "X_encode = tf.one_hot(X, vocab_size)\n",
    "\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(\n",
    "        tf.nn.rnn_cell.GRUCell(num_units = hidden_size, activation=tf.nn.elu),\n",
    "        output_size=vocab_size, activation=tf.nn.elu)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(cell, X_encode, dtype=tf.float32)\n",
    "\n",
    "weights = tf.nn.softmax(outputs)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Final Output Section, Elf Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./project2-elf.ckpt\n",
      "FINAL Prediction\n",
      "------------\n",
      "er watch out, you better not cry, you better not pout. I'm telling you why...\n",
      "\n",
      "Walter finally arrives. He joins his family, watching Jovie.\n",
      "\n",
      "WALTER\n",
      "\n",
      "(to Emily)\n",
      "\n",
      "He wasn't lying.\n",
      "\n",
      "EMILY\n",
      "\n",
      "Merry Christmas.\n",
      "\n",
      "They hug.\n",
      "\n",
      "EXT. CENTRAL PARK - CONTINUOUS\n",
      "\n",
      "Buddy, rusking his life, working on the engine!at high speed. Smoke and sparks billow out. Santa struggles to maintain control. They hit a bump and some toys fly out of the back.\n",
      "\n",
      "A -ack in the Box rattles by Buddy, POPPING OPEN.\n",
      "\n",
      "BUDDY\n",
      "\n",
      "Ah!\n",
      "\n",
      "They find themselves boocked by a giant FOUNTAIN with the Rangers close behind.\n",
      "\n",
      "BUDDY\n",
      "\n",
      "I've almost got it!\n",
      "\n",
      "SANTA\n",
      "\n",
      "We need power, now!\n",
      "\n",
      "Buddy tweaks the engine. It HOWLS TO LIFE and the urge of power BLOWS THE SLEIGH FORTY FEET INTO THE AIR, clearing the fountain.\n",
      "\n",
      "BUDDY\n",
      "\n",
      "(in trirmph)\n",
      "\n",
      "YES!! I DID IT!! I'M THE GREATEST ADOPTED ELF IN THE WHOLE WIDE WORLD!\n",
      "\n",
      "SANTA\n",
      "\n",
      "Good job, Buddy!\n",
      "\n",
      "But now the belly of the coach nails the winged statue atop the fountain, yanking the whole engine back out of the sleigh. YO\n",
      "iter: 9999, loss: 0.438548\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "predicted_sent = []\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./project2-elf.ckpt\")\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X1, y1, batch_size):\n",
    "            X_batch = np.array(X_batch).reshape((batch_size, 25))\n",
    "            y_batch = np.array(y_batch).reshape((batch_size, 25))\n",
    "            tr_op= sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            loss_eval = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            weights_ = weights.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "    y_pred = []\n",
    "    row_idx = 0\n",
    "    for i in range(batch_size):\n",
    "        idx = np.argmax(weights_[i,seq_length-1,:])\n",
    "        y_pred.append(idx)\n",
    "    print ('FINAL Prediction')\n",
    "    print ('------------')\n",
    "    txt = ''.join(ix_to_char[ix] for ix in y_pred)\n",
    "    print(txt)\n",
    "    print ('iter: %d, loss: %f' % (epoch, loss_eval))\n",
    "    print ('------------')\n",
    "    #save_path = saver.save(sess, \"./project2-elf.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
